\chapter{Discussion and Future Work}
\label{cha:conclusion}

\section*{5.1 \hspace{0.25cm} The two proposed methods}
\addcontentsline{toc}{section}{5.1 \hspace{0.25cm} The two proposed methods}
We have developed two high-dimensional predictive models for integrating meta-features. When the meta-features are informative, both models show improvement in prediction performance compared to standard regularized regression, and the improvement increases as the meta-features become more informative. And when the meta-features are irrelevant, the regularized hierarchical model (`xrnet') performs only slightly worse than standard regularized regression, showing robustness in prediction. This robustness can reassure users experimenting with different meta-feature sets, when they are uncertain about their informativeness. We also observe  that for the `xrnet' model, the prediction performance gains are largest  when the number of features are large relative to the number of samples (Figure \ref{fig:sim1}). Thus, although prediction is generally hardest with very high dimensional data, informative meta-features can help compensate for the relatively smaller number of samples. This is an attractive property for practical applications, as in many prognostic and diagnostic studies with genomic data, the sample sizes are often small, in the hundreds of subjects or smaller. The meta-feature guided regularized regression (chapter \ref{cha:xtunecox}) does not exhibit this same behavior. Its performance improvement is constant as the feature set size increases (Figure \ref{fig:sim21}, so it provides stable prediction improvements that only depend on the informativeness of meta-features. 

The two models are also quite different in regards to feature selection. While the hierarchical `xrnet' model performs selection at meta-feature level, the meta-feature guided regularized regression does not. For example, if the meta-features are gene sets/pathways, `xrnet' selects gene sets, i.e., group of genes, rather than individual genes. When standard regularized regression yields a relatively dense model, it hardly provides any insight into disease mechanisms. However, with group of genes that share common biological functions, it reveals association between outcome and those biological functions. Meanwhile, the meta-feature guided regularized regression selects at individual feature level, the same as standard regularized regression. Nevertheless, with individualized penalty for each feature, it usually yields a sparser model. Both of the proposed data integration strategies produce more interpretable model, except in their own way. 

Mathematically, although both methods are based on regularized regression, they are fundamentally different. The regularized hierarchical model integrates meta-features linearly, and models meta-features through the mean of the feature-level regression coefficients $\bm{\beta}$, while the meta-feature guided regularized regression integrates meta-features non-linearly, and models them through the variance of $\bm{\beta}$. Thus they provide truly alternative options for building predictive models with meta-features, and the user may want to try both approaches and choose the better performing one for the problem at hand. Similarities and differences between the two models are summarized below.
\begin{multicols}{2}
\textbf{Regularized hierarchical model}
\begin{itemize}
    \item Prediction improves with informative meta-features
    \item Selection at the meta-feature level
    \item The two penalty parameters are tuned via cross-validation
    \item Models meta-features through the mean of $\bm{\beta}$, linear integration 
\end{itemize}

\columnbreak

\textbf{Meta-feature guided model}
\begin{itemize}
    \item Prediction improves with informative meta-features
    \item Selection at the feature level
    \item Estimates individual penalty parameters via empirical Bayes
    \item Models meta-features through the variance of $\bm{\beta}$, non-linear integration
\end{itemize}
\end{multicols}

The two methods have both been applied to gene expression signatures of breast cancer survival (METABRIC), and anti-PD1 immunotherapy predictive biomarker for melanoma survival. In the METABRIC data application, the two methods yielded different prediction performances. While regularized hierarchical model (`xrnet') showed improvement in prediction after integrating the metagene information, the metagene guided regularized regression did not perform as well as standard regularized regression. This is an example where one method can outperform the other in a specific data application. By contrast, in the anti-PD1 immunotherapy predictive biomarker application, both methods had comparable prediction performance and  both outperformed standard regularized regression. 

\section*{5.2 \hspace{0.25cm} High-dimensionality of genomic data}
\addcontentsline{toc}{section}{5.2 \hspace{0.25cm} High dimensionality of genomic data}
Genomics data are often high dimensional. The human genome contains approximately 3 billion base pairs, which reside in the 23 pairs of chromosomes within the nucleus of all our cells. There are about  20K protein coding genes, 20K non-coding genes in the human genome and 30K CpG islands, so genomewide gene expression and methylation studies have thousands of features. However, the number of samples, especially in oncology settings, is often small relative to the number of features. For cancer patients, tumor genomic profiles are often obtained from  biopsies, which can be highly invasive, risky, and costly. As a result, the number of patients with genomic data can be limited. A typical data set in cancer genomics consists of hundreds to thousands subjects, and the number of genomic variables are in the tens to hundreds of thousands, even millions. 

The novel liquid biopsy techniques, which only requires blood rather than tumor tissue, is less invasive, painful, and much easier to access, and can be performed in most health conditions. This could potentially generate more samples than tissue biopsies, and high dimensionality may no longer be an issue for genomics data. With both samples and features in high dimension, simple linear models are not among the best anymore. Since machine learning has made great progress over the past years, there are better modeling choices for this regime. Non-linear patterns like interactions between features, are hard to detect with limited sample sizes. With large samples, non-linear patterns can be explored with methods like gradient boosting, a tree-based method that specialize in detecting complicated interaction patterns. Deep neural networks, with enough units and layers, can also capture non-linear patterns. These two approaches are hugely successful in settings (e.g., image detection, language translation) where non-linear patterns dominate and where large amounts of samples are available. Liquid biopsies open the possibility to have large sample sizes that can be analyzed using deep learning in the near future, and to have a large impact on cancer genomics. 

\section*{5.3 \hspace{0.25cm} Feature selection}
\addcontentsline{toc}{section}{5.3 \hspace{0.25cm} Feature selection}
So far we have discussed feature selection from a subset of the features already considered in the modeling process. But in high dimensional settings one may want to pre-screening the set of features to be included in the analysis to have more predictive power. A common approach that is not recommended, is to let the data decide the set of features by conducting a pre-analysis based on the data. However, pre-screening features based on the data and then building a predictive model using the same data can result in upwardly biased predictions. What should be done instead, to the degree possible, is to pre-screen based on knowledge from prior studies and the literature. However, this strategy risks missing important features not previously studied. 

The two methods developed in this thesis both use common sparse regularization techniques: the lasso and the elastic net. Both are convex, and have stable and efficient algorithms. Moreover, as we discussed in section \ref{sec:sparse}, the lasso subset selection is consistent under certain conditions, and the elastic net can complement the lasso to deal with group correlated features. However, as convex approximations to best subset selection, they both produce estimators biased toward 0, and there exist some conditions in which their subset selection are not consistent. Discussed in section \ref{sec:nonconvex}, nonconvex regularization, which are closer approximations to best subset selection, produce more parsimonious models, and the estimators are less biased toward 0. The SCAD and the MCP, along with the adaptive lasso, approximation to $L_q$ regularization, enjoy the oracle property; namely, when the true estimators have some zero components, they are estimated as 0 with probability tending to 1, and the nonzero components are estimated as well when the correct submodel is known \citep{fan2001variable}. This property improves the model accuracy compared to the lasso and the elastic net. As the three regularized regression techniques are amenable to be fitted by cyclic coordinate descent, they are natural extensions for the two methods developed in this thesis.

\section*{5.4 \hspace{0.25cm} Inference}
\addcontentsline{toc}{section}{5.4 \hspace{0.25cm} Inference}
Although the approaches developed in this dissertation are conceived mainly for predictive modeling, the two regression methods can also be used in discovery settings to uncover genomic risk factors. However, statistical inference to provide p-values, standard errors and confidence intervals for the model parameter estimates are not readily available. In general, inference is not straightforward for high-dimension since there is no general asymptotic theory comparable to that on which most low-dimensional frequentist inference is based. 

One way to provide inference is to to rely on the Bayesian interpretation of regularized regression, but this requires a change of paradigm as well as relying on computationally expensive MCMC approaches For example, the Bayesian lasso \citep{park2008bayesian},
\begin{displaymath}
\bm{Y}|\bm{\beta}, \lambda,\sigma \sim N(\bm{X\beta}, \sigma^2\bm{I}),
\end{displaymath}
\begin{displaymath}
\bm{\beta}|\lambda,\sigma \sim \prod_{j=1}^p \frac{\lambda}{2\sigma}e^{-\frac{\lambda}{\sigma}|\beta_j|},
\end{displaymath}
The prior distribution of $\bm{\beta}$ is a double exponential distribution (Laplace distribution). Using Bayes rule, the negative log posterior distribution, $\bm{\beta}|\bm{Y},\lambda,\sigma$ is 
\begin{displaymath}
\frac{1}{2\sigma^2} \|\bm{Y}-\bm{X\beta}\|_2^2+\frac{\lambda}{\sigma}\|\bm{\beta}\|_1.
\end{displaymath}
The lasso estimator coincides the mode of the posterior distribution, given $\lambda,\sigma$. With the posterior distribution, we can get not only the point estimator, mode of posterior distribution, but also the variance, which in turn, credible interval. The exact Bayesian calculation for lasso does not have closed form solution, Markov chain Monte Carlo (MCMC) can be used to sample posterior distribution realization. However, the complete Bayesian lasso requires prior distribution for $\sigma$ and $\lambda$, introducing complications. Moreover, MCMC procedure is computationally heavy, especially when the number of features is large, exactly the situation of genomics data. 

Another way of obtaining the variance of estimator is the bootstrp \citep{efron1979bootstrap}. The nonparametric bootstrap is a method of approximating the sampling distributions. It takes resample from the samples (training set) with the same size $n$, with replacement. In the resample, model is trained with cross-validation to get the parameter estimate $\hat{\bm{\beta}}(\lambda_{CV})$. Repeat the procedures B times (enough large times), the bootstrap variance is 
\begin{displaymath}
\text{Var}(\hat{\beta})=\frac{1}{B-1}\sum_{b=1}^B(\hat{\beta}_b^*-\bar{\hat{\beta}}^*)^2,
\end{displaymath}
where $\hat{\beta}_b^*$ is the estimator calculated from the $b^{th}$ resample, and $\bar{\hat{\beta}}^* = \frac{1}{B}\sum_{b=1}^B\hat{\beta}_b^*$ is the mean of resample estimators. Therefore, by central limit theorem, we can get 95\% confidence interval for the random variable $\hat{\bm{\beta}}(\lambda_{CV})$. The nonparametric bootstrap is computationally tractable compared to the Bayesian approach that relies on MCMC, as in each resample iteration, the efficient pathwise coordinate descent algorithm is performed.

Frequentist statistical inference with high dimensional regularized regression is an active research area. Several  recent approaches exist. Among them, referred to as family wise error rate control with the single split method \citep{wasserman2009high}, and its extension multi-split method \citep{meinshausen2009p} are resampling methods based on reducing number of features to manageable size in a hold out set. The idea is to perform sparse regularized regression in one set, which will give an active set of features, $\beta_j:\hat{\beta}_j\neq 0$, with size at most $n$ (sample size). Then inference can be made in the hold out set simply using standard regression with the active features. Because the single split suffers heavily from the randomness of arbitrary split, multi-split with repeated procedure is a natural extension. The summary statistics used for repeated p-values is quantile that will control the family wise error rate under a pre-specified type I error. This line of work offers a potential option for extending our methods from prediction to inference.      
