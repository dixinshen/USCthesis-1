\chapter{Discussion and future work}
\label{cha:conclusion}

\section{Discussion on genomics data integration}
The purpose of this thesis is to develop novel modeling methods predicting health outcomes, based on genomics data. As the types and the volume of genomics data are huge thanks to advanced high-throughput sequencing technologies, as well as the ever-growing annotation databases, there is increased need to integrate multiple types of genomics data, annotation data into modeling process. Because related variables provide more information to the health outcomes of interest, and hence improve prediction performance. The traditional method is modeling one type of genomics data at a time, and combine the models in some form. As to utilizing annotation data, summary statistics, it is usually performed after modeling genomics data. This style of modeling different genomics data separately may ignore the interplay between them, the collective effect on the outcome. In this thesis, we introduced the concept meta-features, the features of the features, to incorporate external data. And we also introduced a meta-feature data matrix $\bm{Z}$ that systematically stores the external data. In the two methods developed in the thesis, chapter \ref{cha:xrnetcox}, \ref{cha:xtunecox}, we mainly discussed how to put annotation information into meta-feature matrix. That is, if we have $p$ genomic features, $q$ functional gene sets (meta-features), the meta-feature matrix $\bm{Z}$ will have dimension $p\times q$, each row represents one genomic feature and has values of 0 or 1 indicating whether this genomic feature belongs to a gene set (1 indicates it belongs to the gene set, and 0 not). However, the usage of meta-feature matrix does not limit to annotation data, in fact, it can accommodate many types of information. We discuss 2 situations to show the flexibility of putting external data into meta-feature matrix $\bm{Z}$.

\begin{itemize}
    \item There are 3 types genomics data, gene expressions, single nucleotide polymorphisms (SNPs), DNA methylation  to be integrated into the modeling process. The meta-feature matrix tells which genomic feature is SNP, gene expression, or methylation. Table \ref{table:d1} shows the indicator meta-feature matrix. For example, ILMN\_343291 is a microarray probe, gene expression; rs10853372 is a SNP locus. 
    \begin{table}[tbh]
    \centering
    \def\arraystretch{1.5}
    \begin{tabular}{|c|c|c|c|}
        \hline
         & \textbf{Gene expression} & \textbf{SNP} & \textbf{Methylation} \\ 
        \specialrule{.1em}{.05em}{.05em}
        ILMN\_343291 & 1 & 0 & 0 \\ 
        \hline
        rs10853372 & 0 & 1 & 0 \\ 
        \hline
        ILMN\_1651210 & 1 & 0 & 0 \\
        \hline
        463100A3 & 0 & 0 & 1 \\
        \hline
        \vdots & \vdots & \vdots & \vdots \\
    \end{tabular}
    \caption{Meta-feature matrix $\bm{Z}$ for multiple types of genomics data}
    \label{table:d1}
    \end{table}
    
    \item There are summary statistics from similar studies on the same set of genomic features. These statistics from meta-analysis can be highly informative. They include p-values, hazard ratios, and source of features. In table \ref{table:d2}, gene BAX has a p\_value 0.0006 associated with the outcome, hazard ratio is 0.7605, the reason being included in the model is from previous GWAS studies. This is a hybrid matrix holding continuous values and indicator values: continuous values like p\-values, hazard ratios gives importance of the features; indicator variable tells the reason why the feature is included. 
    \begin{table}[tbh]
    \centering
    \def\arraystretch{1.5}
    \begin{tabular}{|c|c|c|c|c|c}
        \hline
         & \textbf{p\_value} & \textbf{Hazard ratio} & \textbf{Literature} & \textbf{GWAS}  \\ 
        \specialrule{.1em}{.05em}{.05em}
        BAX & 0.0006 & 0.7605 & 0 & 1 & \dots \\ 
        \hline
        IL6 & 0.2611 & -0.2077 & 1 & 0 & \dots \\ 
        \hline
        LDHB & $8.78\times 10^{-6}$ & 0.0768 & 0 & 1 & \dots \\
        \hline
        \vdots & \vdots & \vdots & \vdots & \vdots & $\ddots$ \\
    \end{tabular}
    \caption{Meta-feature matrix $\bm{Z}$ for summary statistics}
    \label{table:d2}
    \end{table}
\end{itemize}

With the above examples, we are shown the flexibility of the meta-feature matrix housing external information. Through the meta-feature matrix, we can integrate multiple types of genomics data, genomic annotation data, summary statistics from similar studies, and so on. It is the heart of our modeling process to integrate extra information that might be useful to prediction.

\section{Discussion on high dimensionality of genomics data}
Most of the genomics data are high dimensional. The human genome contains approximately 3 billion base pairs, which reside in the 23 pairs of chromosomes within the nucleus of all our cells. Each chromosome contains hundreds to thousands of genes, which makes around 30,000 genes in the human genome. SNPs are common genetic variants happens roughly 1\% of the times among 3 billion base pairs in the human genome, so there are about 10 million SNPs. Over the years, many SNPs have been found. The number of genomics features, e.g., gene expressions, SNPs, DNA methylation, is huge by nature. However, the number of samples, especially in oncology setting, is small relative to the amount of genomics. For cancer patients, genomics data are obtained by tissue biopsy, which can be highly invasive, risky, costly. Some tumor locations are hard to access. And cancer patients are usually under serious health conditions which also prevent them from tissue biopsy. As a result, the number of patients with genomics data is limited. A typical data set in cancer genomics consists of hundreds to thousands subjects, and the number of genomic variables is tens to hundreds thousands, even millions. This makes the data ultra high dimensional ($p>>n$). We have already discussed in high dimensional setting, regularized regression as a linear model, is one of the better options, since every variable contributes a little to none effect to the outcome of interest. Intuitively, considering coming up a surface to classify samples in a high dimensional space, it is easy to use a hyperplane than a complicated non-linear surface. It is the opposite in a relatively low dimensional space, where non-linear pattern is favored. 

Recently, a novel technique, liquid biopsy which takes the human blood sample instead of tumor tissue, has been explored. It is less invasive, painful, easy to access, and can be performed in most health conditions. This could potentially generate more samples than tissue biopsies, and high dimensionality may no longer be an issue for genomics data. With both samples and features in high dimension, simple models such as linear models are not among the best any more. As machine learning has made great progress over the past years, there are better modeling choices. Reconsidering the claim that each genomic feature contribute a little to none effect to the outcome. While this is true, there could also be non-linear patterns like interactions between features that for example, over expression of one gene could cause other genes' expression change. Under limited samples, these complicated patterns are impossible to detect due to lack of information. However, with more samples, it is possible, and there are better models for exploring non-linear patterns. Gradient boosting machine is a tree-based method that specializing in detecting complicated interaction patterns. Deep neural network, with enough units and layers, can mimic any non-linear patterns. These two methods are hugely successful recently, because almost every data has some form of non-linear pattern. And we do see they need large amount of instances/samples to be successful. Liquid biopsy provides us the possibility to have large sample size while the number of genomic features remains large. Although it is not fully validated yet, in the near future, it can be expected to make impact on cancer genomics. As in cancer genomics, complicated non-linear pattern like square, higher order polynomials are hard to interpret the effects of genomics. Gene-gene interactions, gene-environment interactions, or even higher order interactions are of great interest. Therefore, in the case of having plenty of samples, developing methods to integrate genomics data for gradient boosting machine is a meaningful future work.

\section {Discussion on feature selection}
Up to now, the feature selection that we talked about is selecting a subset of the features that already included in the modeling process. Precisely speaking, this process is subset selection, which is to produce a parsimonious model that can reveal the underlying association between features and outcomes. Before further discussing subset selection, we first talk about feature selection, the process of selecting related features to build a predictive model. The predictive modeling process starts with laying out the question: what is the purpose of the predictive model, what type of outcome is to be predicted. Based on the scientific questions, we choose the data to collect that will give the most predictive power. In cancer genomics, data from tissue biopsy may contain different types of genomics data. It is not much of a problem to decide the types of data to be used based on the purpose, but rather, within one type of data, what subset of features to be included in the model. Because of the high dimensionality of data, one might want to narrow down the set of genomic features that have more predictive power. One common approach that is not recommended is to let the data decide the set of features by conducting pre-analysis based on the data. Because pre-screening the features based on the data and building predictive model afterwards is equivalent to using the data twice for modeling. It is problematic to train the model twice on different training sets. What can be done is utilizing knowledge from past studies, literature. But still, it is not a good idea to pre-screening features at all. The features that are considered not important may multivariately work with other features to be predictive to the outcome. Regularized regression has the ability to exclude unimportant features by shrink them to 0; gradient boosted trees can ignore those features by not even using them as tree nodes. It is better to let the model decide how each feature contributes to prediction.

Now in terms of subset selection in regularized regression, the two methods developed in this thesis both used most common sparse regularization technique, the lasso and the elastic net. Because they are both convex, and have stable and efficient algorithms. Moreover, as we discussed in section \ref{sec:sparse}, the lasso subset selection is consistent under certain conditions, and the elastic net can complement the lasso to deal with group correlated features. However, as convex approximations to best subset selection, they both produce estimators biased toward 0, and there exist some conditions in which their subset selection are not consistent. Discussed in section \ref{sec:nonconvex}, nonconvex regularizations, which are more closer approximations to best subset selection, produce more parsimonious model, and the estimators are less biased toward 0. The 2 nonconvex regularizations that are discussed, the SCAD and the MCP, along with the adaptive lasso, approximation to $L_q (0<q<1)$ regularization, enjoy oracle property; namely, when the true estimators have some zero components, they are estimated as 0 with probability tending to 1, and the nonzero components are estimated as well when the correct submodel is known \citep{fan2001variable}. This property improves the model accuracy compared to the lasso and the elastic net. As the 3 regularization techniques enjoy coordinate descent algorithm, they are natual extensions to the two methods developed in this thesis, for the purpose of better subset selection, in terms of both accuracy and unbiased estimator. 

\section{Discussion on statistical inference}